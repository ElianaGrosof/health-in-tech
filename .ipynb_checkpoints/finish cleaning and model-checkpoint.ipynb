{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import just about everything that I'll need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "df = pd.read_csv(\"survey_cleaner1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Age', 'Gender', 'Country', 'self_employed',\n",
       "       'family_history', 'treatment', 'work_interfere', 'no_employees',\n",
       "       'remote_work', 'tech_company', 'benefits', 'care_options',\n",
       "       'wellness_program', 'seek_help', 'anonymity', 'leave',\n",
       "       'mental_health_consequence', 'phys_health_consequence', 'coworkers',\n",
       "       'supervisor', 'mental_health_interview', 'phys_health_interview',\n",
       "       'mental_vs_physical', 'obs_consequence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median = df[\"Age\"].median()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1253 entries, 0 to 1252\n",
      "Data columns (total 25 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   Unnamed: 0                 1253 non-null   int64 \n",
      " 1   Age                        1253 non-null   int64 \n",
      " 2   Gender                     1253 non-null   object\n",
      " 3   Country                    1253 non-null   object\n",
      " 4   self_employed              1235 non-null   object\n",
      " 5   family_history             1253 non-null   object\n",
      " 6   treatment                  1253 non-null   object\n",
      " 7   work_interfere             990 non-null    object\n",
      " 8   no_employees               1253 non-null   object\n",
      " 9   remote_work                1253 non-null   object\n",
      " 10  tech_company               1253 non-null   object\n",
      " 11  benefits                   1253 non-null   object\n",
      " 12  care_options               1253 non-null   object\n",
      " 13  wellness_program           1253 non-null   object\n",
      " 14  seek_help                  1253 non-null   object\n",
      " 15  anonymity                  1253 non-null   object\n",
      " 16  leave                      1253 non-null   object\n",
      " 17  mental_health_consequence  1253 non-null   object\n",
      " 18  phys_health_consequence    1253 non-null   object\n",
      " 19  coworkers                  1253 non-null   object\n",
      " 20  supervisor                 1253 non-null   object\n",
      " 21  mental_health_interview    1253 non-null   object\n",
      " 22  phys_health_interview      1253 non-null   object\n",
      " 23  mental_vs_physical         1253 non-null   object\n",
      " 24  obs_consequence            1253 non-null   object\n",
      "dtypes: int64(2), object(23)\n",
      "memory usage: 244.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# find the columns with nulls in it\n",
    "df.info()\n",
    "df.work_interfere.isnull().sum() #self employed has 18 null, #work_interfere has 263 null, check out options\n",
    "df.work_interfere.unique()\n",
    "\n",
    "# replace NaNs in work_intefere with \"I Don't Know\"\n",
    "df.work_interfere.fillna(\"Don't Know\",inplace=True)\n",
    "# replace NaN's in self_employed with \"No\" because only 18 null and probably don't \n",
    "df.self_employed.fillna(\"No\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling begins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Encode data with Label Encoder\n",
    "for col in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    list(le.classes_)\n",
    "\n",
    "# Scale \"Age\" to be closer to the others\n",
    "scaler = MinMaxScaler()\n",
    "df['Age'] = scaler.fit_transform(df[['Age']])\n",
    "\n",
    "# Split data into features and label (X, y)\n",
    "y = df.treatment # label/dependent variable\n",
    "X = df.drop(columns=[\"treatment\"]) # features/independent variables\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=56) # 70% training and 30% test\n",
    "\n",
    "# Decision Tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "# Try model on test dataset\n",
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Model - how often was it correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function evaluates:\n",
    "* Classification accuracy: percentage of correct predictions\n",
    "* Null accuracy: accuracy that could be achieved by always predicting the most frequent class\n",
    "* Percentage of ones (treatment=1, did get treatment)\n",
    "* Percentage of zeros (treatment=0, did not get treatment)\n",
    "* Confusion matrix: Table that describes the performance of a classification model\n",
    "    * True Positives (TP): we correctly predicted that the target variable occurred (treatment) i.e. the person did seek treatment for mental health\n",
    "    * True Negatives (TN): we correctly predicted that the target variable DID NOT occur i.e. the person did NOT seek treatment\n",
    "    * False Positives (FP): we predicted that they got treatment, when in fact they did not\n",
    "    * False Negatives (FN): we predicted that they did not get treatment, when in fact they did\n",
    "* False Positive Rate\n",
    "* Precision of Positive value (??)\n",
    "* AUC: the percentage of the ROC plot that is underneath the curve\n",
    "    * 0.9-1 = excellent (A)\n",
    "    * 0.8-0.9 = good (B)\n",
    "    * 0.7-0.8 = okay (C)\n",
    "    * 0.6-0.7 = poor (D)\n",
    "    * 0.5-0.6 = fail (F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, y_test, y_pred, X_test, plot=False):\n",
    "    # Classification accuracy: percentage of correct predictions\n",
    "    class_accuracy = metric.accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', class_accuracy)\n",
    "    \n",
    "    # Null accuracy: accuracy that could be acheived by always predicting the most frequent classs\n",
    "    # examine the class distribution of the testing set (using a Pandas Series method)\n",
    "    print('Null accuracy:\\n', y_test.value_counts())\n",
    "    \n",
    "    # percentage of ones i.e. people who got mental health treatment\n",
    "    print('Percentage of people who got treatment:', y_test.mean())\n",
    "    \n",
    "    # percentage of zeros i.e. people who DID NOT get mental health treatment\n",
    "    print('Percentage of people who did not get treatment:', 1-y_test.mean())\n",
    "    \n",
    "    # don't understand why it's [0:25] - just comparing first 25 responses?\n",
    "    # Comparing the true and predicted response values\n",
    "    print('True:', y_test.values[0:25])\n",
    "    print('Predicted:', y_pred[0:25])\n",
    "    \n",
    "    # Confusion matrix \n",
    "    # save confusion matrix and slice into True Negative, False Positive, False Negative, True Positive\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test,y_pred).ravel()\n",
    "    \n",
    "    # Visualize Confusion Matrix\n",
    "    sns.heatmap(confusion, annot=True, fmt='d')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    # Metrics Computed from a Confusion Matrix\n",
    "    \n",
    "    # Classification Accuracy: How often was the classifier correct, overall?\n",
    "    print('Classification Accuracy:', class_accuracy)\n",
    "    \n",
    "    # Classification Error: How often was the classifier incorrect, overall?\n",
    "    print('Classification Error:', 1-class_accuracy)\n",
    "    \n",
    "    # False Positive Rate\n",
    "    fp_rate = fp / float(tn + fp)\n",
    "    \n",
    "    # Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "    print('Precision:', metrics.precision_score(y_test, y_pred))\n",
    "    \n",
    "    ###########################################\n",
    "    # ROC Curves and Area Under the Curve (AUC)\n",
    "    ###########################################\n",
    "    \n",
    "    # Question: Wouldn't it be nice if we could see how sensitivity and specificity \n",
    "    # are affected by various thresholds, without actually changing the threshold?\n",
    "    # Answer: Plot the ROC curve!\n",
    "    \n",
    "    # AUC score - predictions is second argument, NOT predicted probabilties -- still trying to figure this stuff out\n",
    "    print('AUC Score', metrics.roc_auc_score(y_test, y_pred))\n",
    "    \n",
    "    # calculate cross-validated AUC \n",
    "    print('Cross-validated AUC:', cross_val_score(model, X, y, cv=5, scoring='roc_auc').mean())\n",
    "    \n",
    "    # get predicted PROBABILITIES for treatment = 1\n",
    "    y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    \n",
    "    # AUC is the percentage of the ROC plot that is underneath the curve\n",
    "    # Higher value = better classifier \n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_prob)\n",
    "    \n",
    "    # First argument is true values, second arg is predicted PROBabilties\n",
    "    # we do not use y_pred bc will incorrect results without throwing an error\n",
    "    # roc_curve returns: false positive rate, true positive rate, thresholds\n",
    "    \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_prob)\n",
    "    if plot==True:\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.plot(fpr,tpr, color=\"green\", label=\"ROC curve (area= %0.2f)\", % roc_auc)\n",
    "        plt.plot([0,1],[0,1], color='gray', linestyle='--')\n",
    "        plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "        plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "        \n",
    "        # define a function that accepts a threshholds and prints sensitivity and specificity\n",
    "        \n",
    "        def eval_threshold(threshold):\n",
    "            # Sensitivity: When the actual value is positive, how often is the prediction correct?\n",
    "            # Specificity: When the actual value is negative, how often is the prediction correct?\n",
    "            \n",
    "            print('Specificty for ' + str(threshold) + ':', 1-fpr[thresholds>threshold][-1])\n",
    "            \n",
    "            # One way of setting threshold\n",
    "            predict_mine = np.where(y_prob > 0.50, 1, 0)\n",
    "            confusion = metrics.confusion_matrix(y_test, predict_mine)\n",
    "            print(confusion)\n",
    "            \n",
    "            return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "?? how do the results intersect with that big group of people I identified with UpSetR? If I split it into that group and not that group, how does the model(s) hold up?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
