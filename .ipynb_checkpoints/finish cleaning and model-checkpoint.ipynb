{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import just about everything that I'll need\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "\n",
    "df = pd.read_csv(\"survey_cleaner1.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[\"state\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Age', 'Gender', 'Country', 'self_employed',\n",
       "       'family_history', 'treatment', 'work_interfere', 'no_employees',\n",
       "       'remote_work', 'tech_company', 'benefits', 'care_options',\n",
       "       'wellness_program', 'seek_help', 'anonymity', 'leave',\n",
       "       'mental_health_consequence', 'phys_health_consequence', 'coworkers',\n",
       "       'supervisor', 'mental_health_interview', 'phys_health_interview',\n",
       "       'mental_vs_physical', 'obs_consequence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median = df[\"Age\"].median()\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1253 entries, 0 to 1252\n",
      "Data columns (total 25 columns):\n",
      " #   Column                     Non-Null Count  Dtype \n",
      "---  ------                     --------------  ----- \n",
      " 0   Unnamed: 0                 1253 non-null   int64 \n",
      " 1   Age                        1253 non-null   int64 \n",
      " 2   Gender                     1253 non-null   object\n",
      " 3   Country                    1253 non-null   object\n",
      " 4   self_employed              1235 non-null   object\n",
      " 5   family_history             1253 non-null   object\n",
      " 6   treatment                  1253 non-null   object\n",
      " 7   work_interfere             990 non-null    object\n",
      " 8   no_employees               1253 non-null   object\n",
      " 9   remote_work                1253 non-null   object\n",
      " 10  tech_company               1253 non-null   object\n",
      " 11  benefits                   1253 non-null   object\n",
      " 12  care_options               1253 non-null   object\n",
      " 13  wellness_program           1253 non-null   object\n",
      " 14  seek_help                  1253 non-null   object\n",
      " 15  anonymity                  1253 non-null   object\n",
      " 16  leave                      1253 non-null   object\n",
      " 17  mental_health_consequence  1253 non-null   object\n",
      " 18  phys_health_consequence    1253 non-null   object\n",
      " 19  coworkers                  1253 non-null   object\n",
      " 20  supervisor                 1253 non-null   object\n",
      " 21  mental_health_interview    1253 non-null   object\n",
      " 22  phys_health_interview      1253 non-null   object\n",
      " 23  mental_vs_physical         1253 non-null   object\n",
      " 24  obs_consequence            1253 non-null   object\n",
      "dtypes: int64(2), object(23)\n",
      "memory usage: 244.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# find the columns with nulls in it\n",
    "df.info()\n",
    "df.work_interfere.isnull().sum() #self employed has 18 null, #work_interfere has 263 null, check out options\n",
    "df.work_interfere.unique()\n",
    "\n",
    "# replace NaNs in work_intefere with \"I Don't Know\"\n",
    "df.work_interfere.fillna(\"Don't Know\",inplace=True)\n",
    "# replace NaN's in self_employed with \"No\" because only 18 null and probably don't \n",
    "df.self_employed.fillna(\"No\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling begins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "# Encode data with Label Encoder\n",
    "for col in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col])\n",
    "    list(le.classes_)\n",
    "\n",
    "# Scale \"Age\" to be closer to the others\n",
    "scaler = MinMaxScaler()\n",
    "df['Age'] = scaler.fit_transform(df[['Age']])\n",
    "\n",
    "# Split data into features and label (X, y)\n",
    "y = df.treatment # label/dependent variable\n",
    "X = df.drop(columns=[\"treatment\"]) # features/independent variables\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=56) # 70% training and 30% test\n",
    "\n",
    "# Decision Tree classifier object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree classifer\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "# Try model on test dataset\n",
    "y_predict = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating Model - how often was it correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directly from https://www.kaggle.com/kairosart/machine-learning-for-mental-health-1/\n",
    "\n",
    "def evalClassModel(model, y_test, y_pred_class, plot=False):\n",
    "    #Classification accuracy: percentage of correct predictions\n",
    "    print('Accuracy:', metrics.accuracy_score(y_test, y_pred_class))\n",
    "    \n",
    "    #Null accuracy: accuracy that could be achieved by always predicting the most frequent class\n",
    "    # examine the class distribution of the testing set (using a Pandas Series method)\n",
    "    print('Null accuracy:\\n', y_test.value_counts())\n",
    "    \n",
    "    # calculate the percentage of ones\n",
    "    print('Percentage of ones:', y_test.mean())\n",
    "    \n",
    "    # calculate the percentage of zeros\n",
    "    print('Percentage of zeros:',1 - y_test.mean())\n",
    "    \n",
    "    #Comparing the true and predicted response values\n",
    "    print('True:', y_test.values[0:25])\n",
    "    print('Pred:', y_pred_class[0:25])\n",
    "    \n",
    "    #Conclusion:\n",
    "    #Classification accuracy is the easiest classification metric to understand\n",
    "    #But, it does not tell you the underlying distribution of response values\n",
    "    #And, it does not tell you what \"types\" of errors your classifier is making\n",
    "    \n",
    "    #Confusion matrix\n",
    "    # save confusion matrix and slice into four pieces\n",
    "    confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "    #[row, column]\n",
    "    TP = confusion[1, 1]\n",
    "    TN = confusion[0, 0]\n",
    "    FP = confusion[0, 1]\n",
    "    FN = confusion[1, 0]\n",
    "    \n",
    "    # visualize Confusion Matrix\n",
    "    sns.heatmap(confusion,annot=True,fmt=\"d\") \n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    #Metrics computed from a confusion matrix\n",
    "    #Classification Accuracy: Overall, how often is the classifier correct?\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "    print('Classification Accuracy:', accuracy)\n",
    "    \n",
    "    #Classification Error: Overall, how often is the classifier incorrect?\n",
    "    print('Classification Error:', 1 - metrics.accuracy_score(y_test, y_pred_class))\n",
    "    \n",
    "    #False Positive Rate: When the actual value is negative, how often is the prediction incorrect?\n",
    "    false_positive_rate = FP / float(TN + FP)\n",
    "    print('False Positive Rate:', false_positive_rate)\n",
    "    \n",
    "    #Precision: When a positive value is predicted, how often is the prediction correct?\n",
    "    print('Precision:', metrics.precision_score(y_test, y_pred_class))\n",
    "    \n",
    "    \n",
    "    # IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
    "    print('AUC Score:', metrics.roc_auc_score(y_test, y_pred_class))\n",
    "    \n",
    "    # calculate cross-validated AUC\n",
    "    print('Cross-validated AUC:', cross_val_score(model, X, y, cv=10, scoring='roc_auc').mean())\n",
    "    \n",
    "    ##########################################\n",
    "    #Adjusting the classification threshold\n",
    "    ##########################################\n",
    "    # print the first 10 predicted responses\n",
    "    # 1D array (vector) of binary values (0, 1)\n",
    "    print('First 10 predicted responses:\\n', model.predict(X_test)[0:10])\n",
    "\n",
    "    # print the first 10 predicted probabilities of class membership\n",
    "    print('First 10 predicted probabilities of class members:\\n', model.predict_proba(X_test)[0:10])\n",
    "\n",
    "    # print the first 10 predicted probabilities for class 1\n",
    "    model.predict_proba(X_test)[0:10, 1]\n",
    "    \n",
    "    # store the predicted probabilities for class 1\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    if plot == True:\n",
    "        # histogram of predicted probabilities\n",
    "        # adjust the font size \n",
    "        plt.rcParams['font.size'] = 12\n",
    "        # 8 bins\n",
    "        plt.hist(y_pred_prob, bins=8)\n",
    "        \n",
    "        # x-axis limit from 0 to 1\n",
    "        plt.xlim(0,1)\n",
    "        plt.title('Histogram of predicted probabilities')\n",
    "        plt.xlabel('Predicted probability of treatment')\n",
    "        plt.ylabel('Frequency')\n",
    "    \n",
    "    \n",
    "    # predict treatment if the predicted probability is greater than 0.3\n",
    "    # it will return 1 for all values above 0.3 and 0 otherwise\n",
    "    # results are 2D so we slice out the first column\n",
    "    y_pred_prob = y_pred_prob.reshape(-1,1) \n",
    "    y_pred_class = binarize(y_pred_prob, 0.3)[0]\n",
    "    \n",
    "    # print the first 10 predicted probabilities\n",
    "    print('First 10 predicted probabilities:\\n', y_pred_prob[0:10])\n",
    "    \n",
    "    ##########################################\n",
    "    #ROC Curves and Area Under the Curve (AUC)\n",
    "    ##########################################\n",
    "    \n",
    "    #Question: Wouldn't it be nice if we could see how sensitivity and specificity are affected by various thresholds, without actually changing the threshold?\n",
    "    #Answer: Plot the ROC curve!\n",
    "    \n",
    "    \n",
    "    #AUC is the percentage of the ROC plot that is underneath the curve\n",
    "    #Higher value = better classifier\n",
    "    roc_auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "    \n",
    "    \n",
    "\n",
    "    # IMPORTANT: first argument is true values, second argument is predicted probabilities\n",
    "    # we pass y_test and y_pred_prob\n",
    "    # we do not use y_pred_class, because it will give incorrect results without generating an error\n",
    "    # roc_curve returns 3 objects fpr, tpr, thresholds\n",
    "    # fpr: false positive rate\n",
    "    # tpr: true positive rate\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "    if plot == True:\n",
    "        plt.figure()\n",
    "        \n",
    "        plt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "        plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.0])\n",
    "        plt.rcParams['font.size'] = 12\n",
    "        plt.title('ROC curve for treatment classifier')\n",
    "        plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "        plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "    \n",
    "    # define a function that accepts a threshold and prints sensitivity and specificity\n",
    "    def evaluate_threshold(threshold):\n",
    "        #Sensitivity: When the actual value is positive, how often is the prediction correct?\n",
    "        #Specificity: When the actual value is negative, how often is the prediction correct?print('Sensitivity for ' + str(threshold) + ' :', tpr[thresholds > threshold][-1])\n",
    "        print('Specificity for ' + str(threshold) + ' :', 1 - fpr[thresholds > threshold][-1])\n",
    "\n",
    "    # One way of setting threshold\n",
    "    predict_mine = np.where(y_pred_prob > 0.50, 1, 0)\n",
    "    confusion = metrics.confusion_matrix(y_test, predict_mine)\n",
    "    print(confusion)\n",
    "    \n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-a25c38b2cb47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
